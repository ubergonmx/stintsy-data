{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YieAdXLRGvwb"
   },
   "source": [
    "**<span style=\"color:#448844\">Note</span>** This notebook is meant to be interactive. Launch this notebook in Jupyter to see its full potential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4sYOp8FGvwf"
   },
   "source": [
    "Name:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HZGZn5qGvwg"
   },
   "source": [
    "Section:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KF9Gln31Gvwh"
   },
   "source": [
    "# Support Vector Machines Exercise\n",
    "This exercise will guide you in implementing Support Vector Machines (SVM). At the end, you will also see the effect of hyperparameters on your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyxlxkZQGvwh"
   },
   "source": [
    "## Instructions\n",
    "* Read each cell and implement the TODOs sequentially. The markdown/text cells also contain instructions which you need to follow to get the whole notebook working.\n",
    "* Do not change the variable names unless the instructor allows you to.\n",
    "* Answer all the markdown/text cells with \"A: \" on them. The answer must strictly consume one line only.\n",
    "* You are expected to search how to some functions work on the Internet or via the docs. \n",
    "* There are commented markdown cells that have crumbs. Do not delete them or separate them from the cell originally directly below it.  \n",
    "* You may add new cells for \"scrap work\" as long as the crumbs are not separated from the cell below it.\n",
    "* The notebooks will undergo a \"Restart and Run All\" command, so make sure that your code is working properly.\n",
    "* You are expected to understand the data set loading and processing separately from this class.\n",
    "* You may not reproduce this notebook or share them to anyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1630726232785,
     "user": {
      "displayName": "Courtney Ngo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwHigL8at_w8Ufh9uSkYctLRrgfkSzIGqGqcj9Zg=s64",
      "userId": "01361140591205022195"
     },
     "user_tz": -480
    },
    "id": "nq0RKm_xGvwj",
    "outputId": "959a2a0e-dc18-413a-bdb8-00a2da46c116"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This notebook will follow this format:__\n",
    "* We will create toy datasets to try out SVM models with different kernel types\n",
    "* We will see how we can regularize our SVM model\n",
    "* We will train, validate, and test our SVM model with the breast cancer dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5JbXu7yGvwk"
   },
   "source": [
    "# Creating an SVM model with different kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ua_WfzHFGvwk"
   },
   "source": [
    "## Generating a linearly separable dataset\n",
    "Let's create a linearly separable dataset before we get to more difficult datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1630726233681,
     "user": {
      "displayName": "Courtney Ngo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwHigL8at_w8Ufh9uSkYctLRrgfkSzIGqGqcj9Zg=s64",
      "userId": "01361140591205022195"
     },
     "user_tz": -480
    },
    "id": "wvy7daMTGvwl",
    "outputId": "42261fe8-445b-4824-caaa-61afee92ff4b"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "np.random.seed(42)\n",
    "\n",
    "centers = [[1, 2], [-1, -4]]\n",
    "\n",
    "X_linear, y_linear = make_blobs(n_samples=100, centers=centers)\n",
    "y_linear[y_linear == 0] = -1\n",
    "\n",
    "plt.scatter(X_linear[:,0], X_linear[:,1],c=y_linear)\n",
    "print(\"Shape of X_linear\", X_linear.shape)\n",
    "print(\"Shape of y_linear\", y_linear.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSJKqrV-Gvwo"
   },
   "source": [
    "## Train an SVM model\n",
    "We will use `sklearn`'s `SVC` model from the `svm` package. To start, create an SVM model with a __linear kernel.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "# svm.SVC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1630726233683,
     "user": {
      "displayName": "Courtney Ngo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwHigL8at_w8Ufh9uSkYctLRrgfkSzIGqGqcj9Zg=s64",
      "userId": "01361140591205022195"
     },
     "user_tz": -480
    },
    "id": "hMj4EMeWGvwp",
    "outputId": "8922315a-8a2a-44c7-ccb9-ec2cd9a82924"
   },
   "outputs": [],
   "source": [
    "# write code here\n",
    "svc_linear = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train it with our `X_linear` and `y_linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here\n",
    "predictions = None\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be computing for the accuracy multiple times in this notebook, so let's create a function for this.\n",
    "\n",
    "`compute_accuracy()` will compute for the accuracy given two vectors of equal length\n",
    "\n",
    "__Inputs:__\n",
    "- `predictions`: A numpy array of shape `(N,)` consisting of `N` samples representing the predicted values\n",
    "- `actual`: A numpy array of shape `(N,)` consisting of `N` samples representing the actual (target) values\n",
    "\n",
    "__Outputs:__\n",
    "- `accuracy`: A scalar representing the percentage of elements where `predictions` and `actual` match out of the total number of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions, actual):\n",
    "    # write code here\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear SVC accuracy: \", compute_accuracy(predictions, y_linear),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TRViGj1Gvwq"
   },
   "source": [
    "**Sanity Check:** This is a linearly separable data, so linear kernel should get a 100% accuracy here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nq1s2S0WGvwq"
   },
   "source": [
    "## Visualize our model\n",
    "Let's try to visualize the decision boundary and the margin. To do this, we need to get the __weights__ and __bias/y-intercept__ of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the weights of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here\n",
    "W = None\n",
    "\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the bias from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here\n",
    "b = None\n",
    "\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will plot our decision boundary (using `W` and `b`) and the fat margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "executionInfo": {
     "elapsed": 719,
     "status": "ok",
     "timestamp": 1630726234391,
     "user": {
      "displayName": "Courtney Ngo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwHigL8at_w8Ufh9uSkYctLRrgfkSzIGqGqcj9Zg=s64",
      "userId": "01361140591205022195"
     },
     "user_tz": -480
    },
    "id": "hoYZmo8hGvws",
    "outputId": "f203243a-95d8-4678-9a7f-2f3437e8cd7d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "# plotting our data\n",
    "plt.scatter(X_linear[:,0], X_linear[:,1],c=y_linear)\n",
    "\n",
    "# plot decision boundary for 2D case\n",
    "x_1 = np.min(X_linear[:,0])\n",
    "y_1 = (-b - W[0]*x_1) / W[1]\n",
    "x_2 = np.max(X_linear[:,0])\n",
    "y_2 = (-b - W[0]*x_2) / W[1]\n",
    "\n",
    "plt.plot([x_1, x_2], [y_1, y_2],'g',label=\"Decision Boundary\")\n",
    "\n",
    "# plot margins\n",
    "x_1_a = np.min(X_linear[:,0])\n",
    "y_1_a = (1 - b - W[0]*x_1) / W[1]\n",
    "x_2_a = np.max(X_linear[:,0])\n",
    "y_2_a = (1 - b - W[0]*x_2) / W[1]\n",
    "plt.plot([x_1_a, x_2_a], [y_1_a, y_2_a],'--m')\n",
    "\n",
    "x_1_b = np.min(X_linear[:,0])\n",
    "y_1_b = (-1 - b - W[0]*x_1) / W[1]\n",
    "x_2_b = np.max(X_linear[:,0])\n",
    "y_2_b = (-1 - b - W[0]*x_2) / W[1]\n",
    "plt.plot([x_1_b, x_2_b], [y_1_b, y_2_b],'--m', label=\"Margins\")\n",
    "plt.legend()\n",
    "\n",
    "plt.title(\"Linear SVM\")\n",
    "\n",
    "plt.savefig(\"SVM linear with decision boundary.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRhMhj8HGvwt"
   },
   "source": [
    "**Sanity Check:** You should see the boundary (green line) clearly cutting the data with a large margin on either side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F22oZ2l5Gvwt"
   },
   "source": [
    "## Support vectors\n",
    "From the lecture, we learned that SVM retains the training instances which \"define\" the boundary and uses them to predict the new instance's class. Let's see those training instances which SVMs refer to as **support vectors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the number of support vectors in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1630726234393,
     "user": {
      "displayName": "Courtney Ngo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwHigL8at_w8Ufh9uSkYctLRrgfkSzIGqGqcj9Zg=s64",
      "userId": "01361140591205022195"
     },
     "user_tz": -480
    },
    "id": "vLZyUtEtGvwt",
    "outputId": "b7ea2d1e-aae9-402e-d5dc-303928248f3f"
   },
   "outputs": [],
   "source": [
    "# write code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBLqoCh1Gvwu"
   },
   "source": [
    "**Sanity Check:** You should see an array of two numbers. The numbers will tell you the number of support vectors for both classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question #1:__ How many support vectors did the model need in total?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--crumb;qna;Question: How many support vectors did the model need in total?-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the features of the actual support vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1630726234394,
     "user": {
      "displayName": "Courtney Ngo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwHigL8at_w8Ufh9uSkYctLRrgfkSzIGqGqcj9Zg=s64",
      "userId": "01361140591205022195"
     },
     "user_tz": -480
    },
    "id": "fhshIc_dGvwu",
    "outputId": "41318158-ff7e-48f5-c961-f730d0627ad6"
   },
   "outputs": [],
   "source": [
    "# write code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:** You should see the coordinates/feature values of the chosen support vectors. Verify their positions in the visualization above. They should be the nearest points to the margin.\n",
    "\n",
    "For the questions below, feel free to modify the visualization code above to plot the support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question #2:__ How many support vectors does the class in `yellow` need?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--crumb;qna;Question: How many support vectors does the class in yellow need?-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question #3:__ How many support vectors does the class in `violet` need?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--crumb;qna;Question: How many support vectors does the class in violet need?-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIHEyXgEGvww"
   },
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s233T1zQGvww"
   },
   "source": [
    "# Different kernels\n",
    "We can extend SVM's to produce non-linear decision boundaries through kernels. This is similar to the feature transform that you did for the polynomial regression. The difference is that kernels gives you a way to get the same output without explicitly performing the feature transform (which may be expensive to compute specially for very high dimensional transforms). It can even represent an infinite dimensional transform such as the Gaussian / Radial Basis Function (RBF) kernel which _in theory_ can linearly separate any data. However, without proper tuning and regularization, we risk overfitting to the training data which makes your classifier useless since it cannot generalize to unseen data.\n",
    "\n",
    "To apply kernels, we simply replace all instances of the inner product $\\langle \\cdot,\\cdot \\rangle$ with the kernel $K(\\cdot,\\cdot)$. \n",
    "\n",
    "Note that $$W = \\sum_{i=1}^N \\alpha_i y_i x_i$$\n",
    "\n",
    "which implies that $$f(z) = W^Tz+b = \\sum_{i=1}^N \\alpha_i y_i \\langle x_i, z\\rangle +b$$ \n",
    "\n",
    "So you will need to modify the some of the functions to apply the kernel.\n",
    "\n",
    "Some commonly used kernels:\n",
    "\n",
    "- __Gaussian / Radial Basis Function (RBF) kernel__: $K(x,z) = \\exp \\bigl( -\\frac{\\Vert x-z \\Vert^2}{2\\sigma^2}\\bigr)$\n",
    "- __Polynomial kernel__: $K(x,z) = (x^Tz+c)^d$, where $d$ is the degree of the polynomial and $c$ is a hyperparameter set by the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVw7X2GNGvww"
   },
   "source": [
    "## Radial basis function kernel\n",
    "### Generating a non-linearly separable dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "executionInfo": {
     "elapsed": 731,
     "status": "ok",
     "timestamp": 1630726235115,
     "user": {
      "displayName": "Courtney Ngo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwHigL8at_w8Ufh9uSkYctLRrgfkSzIGqGqcj9Zg=s64",
      "userId": "01361140591205022195"
     },
     "user_tz": -480
    },
    "id": "8UeVjRmHGvww",
    "outputId": "371ce8a6-e72a-4b16-f8b3-fc62da740c07"
   },
   "outputs": [],
   "source": [
    "def generate_dummy_circle_data(num_points):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    r = np.random.uniform(0,0.2,num_points)\n",
    "    theta = np.random.uniform(0,2*np.pi,num_points)\n",
    "    inner_circle = np.array([r*np.sin(theta), r*np.cos(theta)]).T\n",
    "    \n",
    "    r = np.random.uniform(0.5,0.7,num_points)\n",
    "    theta = 2*np.pi*np.arange(num_points)/num_points\n",
    "    outer_circle = np.array([r*np.sin(theta), r*np.cos(theta)]).T\n",
    "\n",
    "    X = np.concatenate((inner_circle,outer_circle),axis=0)\n",
    "    y = np.concatenate((np.ones(num_points), np.zeros(num_points)),axis=0)\n",
    "    \n",
    "    randIdx = np.arange(X.shape[0])\n",
    "    np.random.shuffle(randIdx)\n",
    "    \n",
    "    X = X[randIdx]\n",
    "    y = y[randIdx].astype(int)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_circle,y_circle = generate_dummy_circle_data(100)\n",
    "\n",
    "print(\"Shape of X_circle\", X_circle.shape)\n",
    "print(\"Shape of y_circle\", y_circle.shape)\n",
    "\n",
    "plt.scatter(X_circle[:,0],X_circle[:,1],c=y_circle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sR7jaLj8Gvwx"
   },
   "source": [
    "### Train model\n",
    "This data can be easily separated using an `rbf` kernel. Create an `rbf`-kernel SVM, and keep the other parameters to their default values for now. Train it on the circle dataset (`X_circle` and `y_circle`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1630726235116,
     "user": {
      "displayName": "Courtney Ngo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwHigL8at_w8Ufh9uSkYctLRrgfkSzIGqGqcj9Zg=s64",
      "userId": "01361140591205022195"
     },
     "user_tz": -480
    },
    "id": "9pic_Gn5Gvwx",
    "outputId": "3f3ffad2-c8e0-4e92-ad8a-2f550152de33"
   },
   "outputs": [],
   "source": [
    "# write code here\n",
    "svc_rbf = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model predictions on `X_circle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here\n",
    "predictions = None\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then get the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RBF SVC accuracy: \", compute_accuracy(predictions, y_circle),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qf-IQaI1Gvwy"
   },
   "source": [
    "**Sanity check:** Using `rbf` here will give you a perfect accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the number of support vectors per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question #4:__ How many support vectors does the model need?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--crumb;qna;Question: How many support vectors does the model need?-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aD9MEPSRGvwy"
   },
   "source": [
    "### Visualize our model\n",
    "\n",
    "The code below will visualize the SVM's (with an `rbf` kernel) decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "executionInfo": {
     "elapsed": 789,
     "status": "ok",
     "timestamp": 1630726235897,
     "user": {
      "displayName": "Courtney Ngo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwHigL8at_w8Ufh9uSkYctLRrgfkSzIGqGqcj9Zg=s64",
      "userId": "01361140591205022195"
     },
     "user_tz": -480
    },
    "id": "2ci76nzyGvwy",
    "outputId": "2f7acac5-c5ec-4814-a7f9-afadb432d611"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "# visualize the decision boundary\n",
    "x_min, x_max = X_circle[:, 0].min() - 0.2, X_circle[:, 0].max() + 0.2\n",
    "y_min, y_max = X_circle[:, 1].min() - 0.2, X_circle[:, 1].max() + 0.2\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "\n",
    "x_test = np.squeeze(np.stack((xx.ravel(),yy.ravel()))).T\n",
    "\n",
    "Z = svc_rbf.predict(x_test)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z)\n",
    "plt.scatter(X_circle[:, 0], X_circle[:, 1], c=y_circle, edgecolors='black')\n",
    "plt.xlim([x_min,x_max])\n",
    "plt.ylim([y_min,y_max])\n",
    "\n",
    "plt.title(\"RBF kernel SVM\")\n",
    "plt.savefig(\"SVM rbf kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDH2sdlTGvwz"
   },
   "source": [
    "**Sanity Check:** You should see the data cropped in the middle, separating the classes from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7LobKPJGvwz"
   },
   "source": [
    "## Polynomial kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilkD8GyuGvw0"
   },
   "source": [
    "### Generating a polynomial dataset\n",
    "The following cell just creates a dataset that we know is non-linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1630726235899,
     "user": {
      "displayName": "Courtney Ngo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwHigL8at_w8Ufh9uSkYctLRrgfkSzIGqGqcj9Zg=s64",
      "userId": "01361140591205022195"
     },
     "user_tz": -480
    },
    "id": "8_k3Xor-Gvw0",
    "outputId": "597ca0ad-2d3f-4032-f951-6c9e7e9db9ad"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "y1 = [val**2 + np.random.random()*5 for val in np.arange(0,10,1)]\n",
    "y2 = [val**2 + np.random.random()*5 + 30 for val in np.arange(0,10,1)]\n",
    "\n",
    "X_poly = np.zeros((2*10,2))\n",
    "X_poly[:,0] = np.append(np.arange(0,10,1),np.arange(0,10,1))\n",
    "X_poly[:,1] = np.append(y1,y2)\n",
    "\n",
    "y_poly = np.zeros((2*10,))\n",
    "y_poly[0:10] = -1\n",
    "y_poly[10:20] = 1\n",
    "\n",
    "plt.scatter(X_poly[:,0],X_poly[:,1],c=y_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMlsUOS1Gvw0"
   },
   "source": [
    "### Using a polynomial kernel\n",
    "The visualization above is a clear indicator that the data cannot be (properly) separated by a line.\n",
    "\n",
    "Create an SVM model with a `poly` kernel. Set the polynomial degree to `3` for now, $\\gamma$ to 2. Train it with our polynomial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm.SVC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1630726235900,
     "user": {
      "displayName": "Courtney Ngo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwHigL8at_w8Ufh9uSkYctLRrgfkSzIGqGqcj9Zg=s64",
      "userId": "01361140591205022195"
     },
     "user_tz": -480
    },
    "id": "Ca_reEpyGvw1",
    "outputId": "123da46b-c12b-452f-f3a8-7de8e0ce0e62"
   },
   "outputs": [],
   "source": [
    "# write code here\n",
    "svc_poly = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here\n",
    "predictions = None\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute for the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Polynomial SVC accuracy: \", compute_accuracy(predictions, y_poly),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc3raWRJGvw2"
   },
   "source": [
    "### Visualize our model\n",
    "\n",
    "The code below will visualize the polynomial SVM's decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "executionInfo": {
     "elapsed": 733,
     "status": "ok",
     "timestamp": 1630726236624,
     "user": {
      "displayName": "Courtney Ngo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwHigL8at_w8Ufh9uSkYctLRrgfkSzIGqGqcj9Zg=s64",
      "userId": "01361140591205022195"
     },
     "user_tz": -480
    },
    "id": "tIN2OQ05Gvw2",
    "outputId": "d16844c9-bc51-4ac6-9c7c-9a40bcac2b2d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "x_min, x_max = X_poly[:, 0].min() - 0.2, X_poly[:, 0].max() + 0.2\n",
    "y_min, y_max = X_poly[:, 1].min() - 0.2, X_poly[:, 1].max() + 0.2\n",
    "\n",
    "idxPlus=y_poly[y_poly<0]\n",
    "idxMin=y_poly[y_poly>0]\n",
    "plt.scatter(X_poly[:,0],X_poly[:,1],c=y_poly)\n",
    "\n",
    "\n",
    "X2,Y2 = np.mgrid[x_min:x_max:100j,y_min:y_max:100j]\n",
    "Z = svc_poly.decision_function(np.c_[X2.ravel(),Y2.ravel()])\n",
    "Z = Z.reshape(X2.shape)\n",
    "plt.contourf(X2,Y2,Z > 0,alpha=0.4)\n",
    "\n",
    "plt.contour(X2,Y2,Z,colors=['k','k','k'], linestyles=['--','-','--'],levels=[-1,0,1])\n",
    "plt.scatter(svc_poly.support_vectors_[:,0],svc_poly.support_vectors_[:,1],s=120,facecolors='none')\n",
    "plt.scatter(X_poly[:,0],X_poly[:,1],c=y_poly,s=50,alpha=0.95);\n",
    "\n",
    "plt.title(\"Polynomial kernel SVM\")\n",
    "plt.savefig(\"SVM polynomial kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOavppn8Gvw2"
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ue4f0e9Gvw3"
   },
   "source": [
    "# Regularizing our SVM models\n",
    "\n",
    "## Tuning the hyperparameter `degree`\n",
    "We have two regularizers called $\\gamma$ and $C$. But before we get to that, let's try to fit our polynomial `SVC` to our circle dataset with varying values for `degree`.\n",
    "\n",
    "The degrees will be as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_vals = [1, 3, 5, 10, 15, 20]\n",
    "\n",
    "degree_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the decision boundary as gamma increases\n",
    "\n",
    "In the code below, initialize an `SVC` with an `poly` kernel. Set the hyperparameter `degree` to match the `degree` for that iteration.\n",
    "\n",
    "Then fit it to our data `X_circle` and `y_circle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = X_circle[:, 0].min() - 1, X_circle[:, 0].max() + 1\n",
    "y_min, y_max = X_circle[:, 1].min() - 1, X_circle[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "\n",
    "# We will let our classifier predict this set of values\n",
    "x_test = np.squeeze(np.stack((xx.ravel(),yy.ravel()))).T\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt_ctr = 1\n",
    "\n",
    "for degree in degree_vals:\n",
    "\n",
    "    # write code here\n",
    "    clf = None\n",
    "    \n",
    "    \n",
    "    Z = clf.predict(x_test)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.subplot(3,3,plt_ctr)\n",
    "    plt.contourf(xx, yy, Z)\n",
    "    plt.scatter(X_circle[:, 0], X_circle[:, 1], c=y_circle, edgecolors='black')\n",
    "    \n",
    "    plt.xlim([x_min,x_max])\n",
    "    plt.ylim([y_min,y_max])\n",
    "    \n",
    "    plt.title(\"poly kernel, degree=\" + str(degree))\n",
    "    \n",
    "    plt_ctr += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question #5:__ Was the `SVC` with a `poly` kernel able to separate the classes of the circle dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--crumb;qna;Question: Was the SVC with a poly kernel able to separate the classes of the circle dataset?-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our datasets for experiments on $\\gamma$ and $C$ \n",
    "\n",
    "Before we proceed to tuning our $\\gamma$ and $C$ values, let's create a dataset for these experiments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mixed dataset.__ This dataset will have a large overlap between the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1630726236626,
     "user": {
      "displayName": "Courtney Ngo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwHigL8at_w8Ufh9uSkYctLRrgfkSzIGqGqcj9Zg=s64",
      "userId": "01361140591205022195"
     },
     "user_tz": -480
    },
    "id": "5pidTaw7Gvw3",
    "outputId": "4174f675-b62f-4430-e0c6-ba0c654eb247"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "centers = [[0, 0], [1, 0]]\n",
    "X_mixed, y_mixed = make_blobs(n_samples=30, centers=centers)\n",
    "y_mixed[y_mixed == 0] = -1\n",
    "plt.scatter(X_mixed[:,0], X_mixed[:,1],c=y_mixed)\n",
    "\n",
    "print(\"Shape of X_mixed\",X_mixed.shape)\n",
    "print(\"Shape of y_mixed\", y_mixed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is intentionally made to have a large overlap so we can see how our model will change its boundary when we tune our hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Outlier dataset.__ This dataset is the same as the linear dataset (`X_linear`, `y_linear`) but will also contain two outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_outlier = np.concatenate((X_linear, [[-1,-2],[-0.5, -1.75]]), axis=0)\n",
    "y_outlier = np.concatenate((y_linear, [0, 0]))\n",
    "\n",
    "plt.scatter(X_outlier[:,0], X_outlier[:,1], c=y_outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcCrdsU8Gvw4"
   },
   "source": [
    "## Tuning Hyperparameter $\\gamma$ (gamma)\n",
    "\n",
    "From the docs:\n",
    "\n",
    "> The behavior of the model is very sensitive to the gamma parameter. If gamma is too large, the radius of the area of influence of the support vectors only includes the support vector itself and no amount of regularization with C will be able to prevent overfitting.\n",
    "\n",
    "> When gamma is very small, the model is too constrained and cannot capture the complexity or “shape” of the data. The region of influence of any selected support vector would include the whole training set. The resulting model will behave similarly to a linear model with a set of hyperplanes that separate the centers of high density of any pair of two classes.\n",
    "\n",
    "For the following section we will try out the following $\\sigma$ values. The $\\gamma$ = $1/\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_vals = np.arange(0.0001, 0.9, 0.1)\n",
    "sigma_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the decision boundary as $\\gamma$ increases\n",
    "\n",
    "`visualize_gamma_boundary()` will visualize the decision boundary of an `SVC` with varying degrees of $\\sigma$\n",
    "\n",
    "__Inputs:__\n",
    "- `kernel`: could either be `poly`, or `rbf`\n",
    "- `sigma_vals`: A numpy array of shape `(S,)` consisting of `S` numbers representing the $\\sigma$ values we want to try out\n",
    "- `X`: A numpy array of shape `(N, 2)` consisting of `N` samples and `2` dimensions representing the data features `X`\n",
    "- `y`: A numpy array of shape `(N,)` consisting of `N` samples representing the class of each sample\n",
    "\n",
    "\n",
    "The code has been filled up except for the part where the `SVC` is initialized and trained on `X` and `y`. __Your tasks are:__\n",
    "- Initialize an `SVC` with the `kernel`. Do not forget to set `gamma` hyperparameter to the inverse of that iteration's `sigma` value. \n",
    "- Fit the model to the input data `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 955
    },
    "executionInfo": {
     "elapsed": 2308,
     "status": "ok",
     "timestamp": 1630726238916,
     "user": {
      "displayName": "Courtney Ngo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwHigL8at_w8Ufh9uSkYctLRrgfkSzIGqGqcj9Zg=s64",
      "userId": "01361140591205022195"
     },
     "user_tz": -480
    },
    "id": "5_3rofwCGvw5",
    "outputId": "e15f3902-1583-4d56-e95d-ce7298d604c3"
   },
   "outputs": [],
   "source": [
    "def visualize_gamma_boundary(kernel, sigma_vals, X, y):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "\n",
    "    # We will let our classifier predict this set of values\n",
    "    x_test = np.squeeze(np.stack((xx.ravel(),yy.ravel()))).T\n",
    "\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt_ctr = 1\n",
    "\n",
    "    for sigma in sigma_vals:\n",
    "\n",
    "        # write code here\n",
    "        clf = \n",
    "        \n",
    "\n",
    "        Z = clf.predict(x_test)\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        plt.subplot(3,3,plt_ctr)\n",
    "        plt.contourf(xx, yy, Z)\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='black')\n",
    "\n",
    "        plt.xlim([x_min,x_max])\n",
    "        plt.ylim([y_min,y_max])\n",
    "\n",
    "        plt.title(kernel + \" kernel, gamma = \" + \"{:.2f}\".format(1/sigma))\n",
    "\n",
    "        plt_ctr += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mixed dataset__ Let's try to run an SVM with an `rbf` kernel on the mixed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_gamma_boundary(\"rbf\", sigma_vals, X_mixed, y_mixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keaD-1LpGvw5"
   },
   "source": [
    "**Sanity Check:** $\\gamma = 1/\\sigma$\n",
    "\n",
    "As $\\gamma$ increases, the standard deviation decreases. What you should see are blobs with small sizes (small standard deviation/$\\sigma$) when $\\gamma$ is large. As $\\gamma$ decreases (and $\\sigma$ increases), then the blobs look like they are clustering together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Outlier dataset__ We can also check out how $\\gamma$ will handle the outlier dataset with a `poly` kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_gamma_boundary(\"rbf\", sigma_vals, X_outlier, y_outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question #6:__ What range of `gamma` values better fit our outlier data? Smaller or bigger `gamma` values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--crumb;qna;Question: What range of gamma values better fit our outlier data? Smaller or bigger gamma values?-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqF9xfckGvw6"
   },
   "source": [
    "## Tuning Hyperparameter `C`\n",
    "\n",
    "\n",
    "Recall: The dual optimization problem for support vector machines that we want to solve.<br />\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "    \\max_\\alpha & \\quad \\sum_{i=1}^N \\alpha_i - \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N y_i y_j \\alpha_i \\alpha_j \\langle x_i, x_j \\rangle \\\\\n",
    "    \\text{such that} & \\quad 0 \\leq \\alpha_i \\leq C, \\forall_i \\\\\n",
    "    \\quad & \\quad \\sum_{i=1}^N \\alpha_i y_i = 0\n",
    "\\end{array}\n",
    "$$\n",
    "Here, we see that all $\\alpha$'s must not only be more than or equal to 0, we bound it to $0 \\leq \\alpha_i \\leq C$\n",
    "\n",
    "from sklearn's documentation:\n",
    "\n",
    ">The `C` parameter trades off misclassification of training examples against simplicity of the decision surface. A low `C` makes the decision surface smooth, while a high `C` aims at classifying all training examples correctly by giving the model freedom to select more samples as support vectors.\n",
    "\n",
    "Here, you can think of `C` as the inverse of our previous regularization parameter $\\lambda$, so $C = 1/\\lambda$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    " \\alpha_i = 0 & \\implies y_i (W^Tx_i + b) \\geq 1 \\\\\n",
    " \\alpha_i = C & \\implies y_i (W^Tx_i + b) \\leq 1 \\\\\n",
    " 0 < \\alpha_i < C & \\implies y_i (W^Tx_i + b) = 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For the following section we will try out the following `C` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_vals = [0.0001, 0.001, 0.01 , 0.1, 1.0, 1.5, 2.0, 30, 50, 1000, 2000, 5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the decision boundary as gamma increases\n",
    "\n",
    "`visualize_gamma_boundary()` will visualize the decision boundary of an `SVC` with varying degrees of $C$\n",
    "\n",
    "__Inputs:__\n",
    "- `kernel`: could either be `poly`, or `rbf`\n",
    "- `C_vals`: A numpy array of shape `(S,)` consisting of `S` numbers representing the `C` values we want to try out\n",
    "- `X`: A numpy array of shape `(N, 2)` consisting of `N` samples and `2` dimensions representing the data features `X`\n",
    "- `y`: A numpy array of shape `(N,)` consisting of `N` samples representing the class of each sample\n",
    "\n",
    "\n",
    "The code has been filled up except for the part where the `SVC` is initialized and trained on `X` and `y`. __Your tasks are:__\n",
    "- Initialize an `SVC` with the `kernel`. Do not forget to set the `C` hyperparameter to that iteration's `C` value. \n",
    "- Then fit it to the input data `X` and `y`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2500,
     "status": "ok",
     "timestamp": 1630726241407,
     "user": {
      "displayName": "Courtney Ngo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwHigL8at_w8Ufh9uSkYctLRrgfkSzIGqGqcj9Zg=s64",
      "userId": "01361140591205022195"
     },
     "user_tz": -480
    },
    "id": "B91UnTmfGvw6",
    "outputId": "0544f7e4-b49b-4fce-84ca-0d95a034cfda"
   },
   "outputs": [],
   "source": [
    "def visualize_C_boundary(kernel, C_vals, X, y):\n",
    "    # visualize the decision boundary as C varies\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "\n",
    "    # This will be our test set\n",
    "    x_test = np.squeeze(np.stack((xx.ravel(),yy.ravel()))).T\n",
    "\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt_ctr = 1\n",
    "\n",
    "    for C in C_vals:\n",
    "\n",
    "        # write code here\n",
    "        clf = \n",
    "        \n",
    "\n",
    "        Z = clf.predict(x_test)\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        plt.subplot(4,3,plt_ctr)\n",
    "        plt.contourf(xx, yy, Z)\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='black')\n",
    "\n",
    "        plt.xlim([x_min,x_max])\n",
    "        plt.ylim([y_min,y_max])\n",
    "\n",
    "        plt.title(kernel + \" kernel, C = \" + str(C))\n",
    "\n",
    "        plt_ctr += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mixed dataset__ Let's try to run an SVM with an `rbf` kernel on the mixed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_C_boundary(\"rbf\", C_vals, X_mixed, y_mixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZC13TTxMGvw7"
   },
   "source": [
    "**Sanity Check:**\n",
    "\n",
    "You should see a more complex model as you increase C. In the case of SVMswith an `rbf` kernel, you should see more blobs created to section off the the classes (yellows from the violets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Outlier dataset__ Let's try to run an SVM with a `poly` kernel on the outlier dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_C_boundary(\"poly\", C_vals, X_outlier, y_outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning our models \n",
    "\n",
    "For this section, we will use the __Wisconsin breast cancer dataset__. It has `569 instances` and `30 features`. The features are characteristics measured from images of cell nuclei of aspirated breast masses:\n",
    "\n",
    "* radius (mean)\n",
    "* texture (mean)\n",
    "* perimeter (mean)\n",
    "* area (mean)\n",
    "* smoothness (mean)\n",
    "* compactness (mean)\n",
    "* concavity (mean)\n",
    "* concave points (mean)\n",
    "* symmetry (mean)\n",
    "* fractal dimension (mean)\n",
    "* radius (standard error)\n",
    "* texture (standard error)\n",
    "* perimeter (standard error)\n",
    "* area (standard error)\n",
    "* smoothness (standard error)\n",
    "* compactness (standard error)\n",
    "* concavity (standard error)\n",
    "* concave points (standard error)\n",
    "* symmetry (standard error)\n",
    "* fractal dimension (standard error)\n",
    "* radius (worst)\n",
    "* texture (worst)\n",
    "* perimeter (worst)\n",
    "* area (worst)\n",
    "* smoothness (worst)\n",
    "* compactness (worst)\n",
    "* concavity (worst)\n",
    "* concave points (worst)\n",
    "* symmetry (worst)\n",
    "* fractal dimension (worst)\n",
    "\n",
    "Our goal is to determine whether the cell nuclei of the aspirated breast mass is __malignant__ or __benign__.\n",
    "\n",
    "There are `212` malignant, and `357` benign samples. We will be stratifying our dataset when we split our test data to make sure we get the proper representation for our training, validation, and test sets.\n",
    "\n",
    "You can learn more about the dataset here: https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_breast_cancer = datasets.load_breast_cancer()\n",
    "data_breast_cancer.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features `X` is stored in `data_breast_cancer.data`, and the labels `y` is stored in `data_breast_cancer.target`. \n",
    "\n",
    "Accessing `data_breast_cancer.target_names` will return the mapping of the labels, while `data_breast_cancer.feature_names` will return the names of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_breast_cancer.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_breast_cancer.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our dataset\n",
    "Load our `X` and `y` before splitting our train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here\n",
    "X_cancer = None\n",
    "\n",
    "X_cancer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here\n",
    "y_cancer = None\n",
    "\n",
    "y_cancer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split out train and test sets. The test set should be `20%` of the original data, and we have to make sure that the data is stratified. Set the random state to `42` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here\n",
    "X_train, X_test, y_train, y_test = None\n",
    "\n",
    "print(\"X_train shape : \", X_train.shape)\n",
    "print(\"y_train shape : \", y_train.shape)\n",
    "print(\"X_test shape : \", X_test.shape)\n",
    "print(\"y_test shape : \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed, let's have a quick look at our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "temp_df = pd.DataFrame(X_train, columns=data_breast_cancer.feature_names)\n",
    "temp_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above, we can see that the features are scaled differently. We don't want the model to think that one feature is important simply because of the scale of the features, so we will standardize our data before we start modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling our features\n",
    "We will first scale our features before modelling. We will use `StandardScaler` to standardize our features. \n",
    "\n",
    "Standardizing will make sure that each of our features will each have a $\\mu=0$ and a $\\sigma^2=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `StandardScaler` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here\n",
    "scaler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the `scaler` to fit our train data. Then, assign the scaled data to our our newly scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here\n",
    "\n",
    "X_train_scaled = None\n",
    "\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ We should only scale our training data and not our test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question #7:__ Why should we not scale our data before splitting our data into train and test sets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--crumb;qna;Question: Why should we not scale our data before splitting our data into train and test sets?-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could call `scaler.mean_` and `scaler.var_` to get the $\\mu$ and $\\sigma^2$ that was used by our `scaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.var_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we can proceed to modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized search of hyperparameters\n",
    "For this section we will use `RandomizedSearchCV` to tune our hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create our base model. Set the model's `max_iter` to `10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here\n",
    "svc = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the hyperparameters.\n",
    "\n",
    "__Hyperparameters__:\n",
    "- C could be 0.0001, 0.001, 0.01 , 0.1, 1.0, 5, 30, 50\n",
    "- kernel could be linear, poly, rbf\n",
    "- degree could be 1, 3, 5, 10, 25, 50\n",
    "- gamama could be scale, auto, 1000, 10, 5, 2.5, 1.5, 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here\n",
    "hyperparameters= None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize your `RandomizedSearchCV` with these additional parameters:\n",
    "- `30` random models\n",
    "- `5`-fold cross-validation\n",
    "- also set the random state to `42` so we'll get the same results\n",
    "\n",
    "Then fit it to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# write code here\n",
    "rssvc = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the best parameters found by `RandomizedSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question #8:__ What are the best parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--crumb;qna;Question: What are the best parameters?-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also get the results of each randomized model using `rssvc.cv_results_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(rssvc.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here\n",
    "predictions_train = None\n",
    "\n",
    "predictions_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute for the training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here\n",
    "predictions_train = None\n",
    "\n",
    "acc = compute_accuracy(predictions_train, y_train)\n",
    "print(\"Best model train accuracy:\",acc,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing phase\n",
    "\n",
    "Now that we have our best model, let's see how it performs on our test dataset. Before we start predicting, make sure we scale our test data set. Use the `scaler` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here\n",
    "X_test_scaled = None\n",
    "\n",
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get the test predictions and test results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here\n",
    "predictions_test = None\n",
    "\n",
    "acc = compute_accuracy(predictions_test, y_test)\n",
    "print(\"Best model test accuracy:\",acc,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question #9:__ What is the test accuracy of the best model found?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--crumb;qna;Question: What is the test accuracy of the best model found?-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question #10**: Congratulations for making it this far. In your own words (no need to be formal), summarize the biggest learnings, insights, and takeaways that you have for STINTSY. *(You get a point as long as you answer this question)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "* Support vector machines come in different kernels: linear, polynomial, and rbf. There are other kernels that you can also try out or you can also make your own.\n",
    "* The kernels make feature transformation quickly because of the way SVM structured its optimization\n",
    "* While we didn't cover it, support vector machines can also handle regression tasks\n",
    "* SVMs have two regularizers: $\\gamma$ which control the standard deviation of our rbf kernels, and `C` which controls the penalization of a sample being placed in the wrong boundary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IriVUpFpGvw7"
   },
   "source": [
    "## <center>fin</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "YTjBjSaRGvw8"
   },
   "source": [
    "\n",
    "<!-- DO NOT MODIFY OR DELETE THIS -->\n",
    "\n",
    "<sup>made/compiled by daniel stanley tan & courtney anne ngo 🐰 & thomas james tiam-lee</sup> <br>\n",
    "<sup>for comments, corrections, suggestions, please email:</sup><sup> danieltan07@gmail.com & courtneyngo@gmail.com & thomasjamestiamlee@gmail.com</sup><br>\n",
    "<sup>please cc your instructor, too</sup>\n",
    "<!-- DO NOT MODIFY OR DELETE THIS -->"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Support Vector Machines.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
