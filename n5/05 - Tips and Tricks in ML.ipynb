{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fed25b66",
   "metadata": {},
   "source": [
    "You don't have to code anything here, just read and follow along."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93feb8bc",
   "metadata": {},
   "source": [
    "# Tips and Tricks in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae91045",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "* You may not reproduce this notebook or share them to anyone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5918f48c",
   "metadata": {},
   "source": [
    "## Import\n",
    "Import **numpy**, **pandas**, **sklern.preprocessing**, and **matplotlib**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f4e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (5.0, 5.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1fee8b",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Let's generate the data that we will scale. Note that in normalization and standardization, we need to perform it for each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cde96d0",
   "metadata": {},
   "source": [
    "Let's generate random `x` and `y` features for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aebdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x = np.arange(0, 6, 0.03) + np.random.randn(200) * 1.3 + 2\n",
    "y = np.arange(0, 6, 0.03) + np.random.randn(200) * 1.3 + -4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae98581a",
   "metadata": {},
   "source": [
    "Visualize the data in a 2D graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aedc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, 'ko')\n",
    "\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-10, 10)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Original Data')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4a73fd",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Normalization is used to transform features to a similar scale. This is also called the min-max scaling. Perform normalization on our data by following the formula below.\n",
    "\n",
    "$$x_{normalization} = \\frac{x - min(x)}{max(x) - min(x)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f571ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_normalized_own = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "y_normalized_own = (y - np.min(y)) / (np.max(y) - np.min(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee28d24",
   "metadata": {},
   "source": [
    "Visualize the result in a 2D graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40187fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_normalized_own, y_normalized_own, 'ko')\n",
    "\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-10, 10)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Normalized, Zoomed-out')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eda3f58",
   "metadata": {},
   "source": [
    "To visualize the data better, let's zoom-in by changing the limits of the x and y axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acd2827",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_normalized_own, y_normalized_own, 'ko')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Normalized, Zoomed-in')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ee8c44",
   "metadata": {},
   "source": [
    "### Normalization using `sklearn.preprocessing.MinMaxScaler`\n",
    "\n",
    "Let's use `sklearn.preprocessing.MinMaxScaler` to normalize our data. The result should be similar to our own implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e95c6",
   "metadata": {},
   "source": [
    "Instantiate a `MinMaxScaler` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1736a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cbcc3b",
   "metadata": {},
   "source": [
    "Normalize `x` and `y` values by calling the `fit_transform()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199345fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_normalized_sklearn = scaler.fit_transform(x.reshape(-1,1))\n",
    "y_normalized_sklearn = scaler.fit_transform(y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc52df2",
   "metadata": {},
   "source": [
    "Visualize the result in a 2D graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9555df4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_normalized_sklearn, y_normalized_sklearn, 'ko')\n",
    "\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-10, 10)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Normalized, Zoomed-out')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc81cbcd",
   "metadata": {},
   "source": [
    "To visualize the data better, let's zoom-in by changing the limits of the x and y axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e25c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_normalized_sklearn, y_normalized_sklearn, 'ko')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Normalized, Zoomed-in')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c969b51c",
   "metadata": {},
   "source": [
    "Display the graph of our implementation of normalization and `sklearn.preprocessing.MinMaxScaler()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d47de76",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(x_normalized_own, y_normalized_own, 'ko')\n",
    "ax1.set_title('Our Implementation')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.grid()\n",
    "\n",
    "ax2.plot(x_normalized_sklearn, y_normalized_sklearn, 'ko')\n",
    "ax2.set_title('Using sklearn')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2292c0eb",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "\n",
    "Standardization transforms features by subtracting the data from the mean and dividing it by the standard deviation. This is often called the z-score. Perform standardization on our data by following the formula below.\n",
    "\n",
    "$$x_{standardized}=\\frac{x-mean(x)}{stddev(x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9552cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_standardized_own = (x - np.mean(x)) / np.std(x)\n",
    "y_standardized_own = (y - np.mean(y)) / np.std(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a952195",
   "metadata": {},
   "source": [
    "Visualize the result in a 2D graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936455fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_standardized_own, y_standardized_own, 'ko')\n",
    "\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-10, 10)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Standardized, Zoomed-out')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dd0301",
   "metadata": {},
   "source": [
    "To visualize the data better, let's zoom-in by changing the limits of the x and y axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b701d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_standardized_own, y_standardized_own, 'ko')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Standardized, Zoomed-in')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9999d62",
   "metadata": {},
   "source": [
    "Check if the $\\mu = 0$ and $\\sigma = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9b8524",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Feature x:')\n",
    "print('Mean:', '{:.2f}'.format(np.mean(x_standardized_own)))\n",
    "print('Standard deviation:', '{:.2f}\\n'.format(np.std(x_standardized_own)))\n",
    "\n",
    "print('Feature y:')\n",
    "print('Mean:', '{:.2f}'.format(np.mean(y_standardized_own)))\n",
    "print('Standard deviation:', '{:.2f}\\n'.format(np.std(y_standardized_own)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72216d1",
   "metadata": {},
   "source": [
    "### Standardization using `sklearn.preprocessing.StandardScaler`\n",
    "\n",
    "Let's use `sklearn.preprocessing.StandardScaler` to standardize our data. The result should be similar to our own implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e07aed8",
   "metadata": {},
   "source": [
    "Instantiate a `StandardScaler` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b75e978",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler = sklearn.preprocessing.StandardScaler()\n",
    "y_scaler = sklearn.preprocessing.StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd394ab2",
   "metadata": {},
   "source": [
    "Standardize `x` and `y` values by calling the `fit_transform()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0260720",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_standardized_sklearn = x_scaler.fit_transform(x.reshape(-1, 1))\n",
    "y_standardized_sklearn = y_scaler.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c29fd8",
   "metadata": {},
   "source": [
    "Visualize the result in a 2D graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6dac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_standardized_sklearn, y_standardized_sklearn, 'ko')\n",
    "\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-10, 10)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Standardized, Zoomed-out')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ef2e4a",
   "metadata": {},
   "source": [
    "To visualize the data better, let's zoom-in by changing the limits of the x and y axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f44ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_standardized_sklearn, y_standardized_sklearn, 'ko')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Standardized, Zoomed-in')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b317ae27",
   "metadata": {},
   "source": [
    "Check if the $\\mu = 0$ and $\\sigma = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414f730",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Feature x:')\n",
    "print('Mean:', '{:.2f}'.format(np.mean(x_standardized_sklearn)))\n",
    "print('Standard deviation:', '{:.2f}\\n'.format(np.std(x_standardized_sklearn)))\n",
    "\n",
    "print('Feature y:')\n",
    "print('Mean:', '{:.2f}'.format(np.mean(y_standardized_sklearn)))\n",
    "print('Standard deviation:', '{:.2f}\\n'.format(np.std(y_standardized_sklearn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdd3753",
   "metadata": {},
   "source": [
    "Display the graph of our implementation of normalization and `sklearn.preprocessing.StandardScaler()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a566a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(x_standardized_own, y_standardized_own, 'ko')\n",
    "ax1.set_title('Our Implementation')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.grid()\n",
    "\n",
    "ax2.plot(x_standardized_sklearn, y_standardized_sklearn, 'ko')\n",
    "ax2.set_title('Using sklearn')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32096554",
   "metadata": {},
   "source": [
    "## Feature Encoding\n",
    "\n",
    "Let's create a synthetic dataset for this section. The dataset is composed of 3 features, namely `size`, `color`, and `type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec97dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clothes = pd.DataFrame(columns=['size', 'color', 'type'])\n",
    "\n",
    "clothes['size'] = ['medium', 'large', 'small', 'medium', 'extra large', \n",
    "                   'large', 'medium', 'extra small', 'medium', 'large']\n",
    "\n",
    "clothes['color']= ['red', 'green', 'blue', 'white', 'gray', 'black', \n",
    "                   'green', 'blue', 'grey', 'green']\n",
    "\n",
    "clothes['type'] = ['rayon', 'polyester', 'cotton', 'cotton', 'cotton', \n",
    "                   'polyester', 'rayon', 'linen', 'cotton', 'polyester']\n",
    "\n",
    "clothes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06cc341",
   "metadata": {},
   "source": [
    "### Label Encoding via `sklearn.preprocessing.LabelEncoder`\n",
    "\n",
    "Let's use `sklearn.preprocessing.LabelEncoder` to encode our labels with value between `0` to `num_classes - 1`, where `num_classes` equals the number of classes in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde9ed75",
   "metadata": {},
   "source": [
    "Instantiate a `LabelEncoder` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5cfdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = sklearn.preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9069e2",
   "metadata": {},
   "source": [
    "Fit the `type` feature by calling the `fit()` function of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f6e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder.fit(clothes['type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd65e6b3",
   "metadata": {},
   "source": [
    "Display the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691516e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0729eef0",
   "metadata": {},
   "source": [
    "Thus, labels will be transformed from string values to their corresponding integer values:\n",
    "- `cotton` - `0`\n",
    "- `linen` - `1`\n",
    "- `polyester` - `2`\n",
    "- `rayon` - `3`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9860eb5",
   "metadata": {},
   "source": [
    "Transform the `type` feature by calling the `transform()` function of the object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0562720",
   "metadata": {},
   "outputs": [],
   "source": [
    "clothes['type'] = label_encoder.transform(clothes['type'])\n",
    "clothes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a253063f",
   "metadata": {},
   "source": [
    "To reverse the encoding, call the `inverse_transform()` function of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b0319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clothes['type'] = label_encoder.inverse_transform(clothes['type'])\n",
    "clothes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff12787",
   "metadata": {},
   "source": [
    "We set it back to our original categorical data because label encoding is not a suitable preprocessing technique for the `type` column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7ef863",
   "metadata": {},
   "source": [
    "### One-Hot Encoding via `sklearn.preprocessing.OneHotEncoder`\n",
    "\n",
    "Let's use `sklearn.preprocessing.OneHotEncoder` to encode our categorical features as a one-hot numeric array.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7129f2dc",
   "metadata": {},
   "source": [
    "Instantiate a `OneHotEncoder` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e8d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder = sklearn.preprocessing.OneHotEncoder(dtype='int8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c2812e",
   "metadata": {},
   "source": [
    "Fit the `type` feature by calling the `fit()` function of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e3d79e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "one_hot_encoder.fit(clothes['type'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45097dde",
   "metadata": {},
   "source": [
    "Display the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1045295",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32ba9bd",
   "metadata": {},
   "source": [
    "The encoding will then be an array with 4 columns, where the columns represents:\n",
    "- column 1 - `cotton`\n",
    "- column 2 - `linen`\n",
    "- column 3 - `polyester`\n",
    "- column 4 - `rayon`\n",
    "\n",
    "If the instance has a value `linen` for the `type` feature, then the one-hot encoding for this instance is `[0, 1, 0, 0]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa42dca",
   "metadata": {},
   "source": [
    "Transform the `type` feature by calling the `transform()` function of the object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaedb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = one_hot_encoder.transform(clothes['type'].values.reshape(-1, 1)).toarray()\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1c5f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_df = pd.DataFrame(encoding, columns=[x for x in one_hot_encoder.categories_] )\n",
    "\n",
    "clothes = clothes.drop(['type'], axis=1) \n",
    "clothes = pd.concat([clothes, type_df], axis=1)\n",
    "clothes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf408877",
   "metadata": {},
   "source": [
    "We'll rename the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef2a36c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clothes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b3bf5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clothes.columns=['size', 'color', 'cotton', 'linen', 'polyester', 'rayon']\n",
    "clothes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a791ab",
   "metadata": {},
   "source": [
    "### Ordinal Encoding\n",
    "\n",
    "Ordinal encoding is a type of label encoding where there is an order associated with the data. In our example, the `size` feature is ordinal.\n",
    "\n",
    "Let's create a dictionary that will map string values in the `size` feature to its corresponding integer value according to some order. See list below:\n",
    "- `extra small` - `0`\n",
    "- `small` - `1`\n",
    "- `medium` - `2`\n",
    "- `large` - `3`\n",
    "- `extra large` - `4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d335d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clothes_sizes_dict= {\n",
    "    'extra small' : 0,\n",
    "    'small' : 1,\n",
    "    'medium' : 2,\n",
    "    'large' : 3,\n",
    "    'extra large' : 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508b35bf",
   "metadata": {},
   "source": [
    "Use the `map()` function to transform the `size` feature to its corresponding ordinal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0de34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clothes['size'] = clothes['size'].map(clothes_sizes_dict)\n",
    "clothes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520c2af4",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Data augmentation is helpful especially when training your machine learning model. This makes your model robust to different variations of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da90bbc",
   "metadata": {},
   "source": [
    "Import necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f4ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from skimage.util import random_noise\n",
    "from skimage.filters import gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2dce86",
   "metadata": {},
   "source": [
    "Read the image and display its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03de280f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image = io.imread('https://i.imgur.com/DHI2jdW.png')\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8809a5",
   "metadata": {},
   "source": [
    "Display the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c833800",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d15c26d",
   "metadata": {},
   "source": [
    "### Rotation\n",
    "\n",
    "Rotate the image by some degree. In the example below, we rotated the image by 45 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7524a133",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_image = rotate(image, angle=45, mode='wrap')\n",
    "\n",
    "plt.imshow(rotated_image)\n",
    "plt.title('Rotated Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7339b659",
   "metadata": {},
   "source": [
    "### Translation\n",
    "\n",
    "Translate the image by some pixel. In the example below, we moved the image to 50 pixels upwards and 50 pixels to the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d75b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = AffineTransform(translation=(50, 50))\n",
    "translated = warp(image, transform, mode='wrap')\n",
    "\n",
    "plt.imshow(translated)\n",
    "plt.title('Translated Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a08a72",
   "metadata": {},
   "source": [
    "### Horizontal Flip\n",
    "\n",
    "Flip the image with respect to the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7420dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped_image = np.fliplr(image)\n",
    "\n",
    "plt.imshow(flipped_image)\n",
    "plt.title('Horizontally-flipped Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a2dc98",
   "metadata": {},
   "source": [
    "### Vertical Flip\n",
    "\n",
    "Flip the image with respect to the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9480102",
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped_image = np.flipud(image)\n",
    "\n",
    "plt.imshow(flipped_image)\n",
    "plt.title('Vertically-flipped Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b991e8",
   "metadata": {},
   "source": [
    "### Random Noise\n",
    "\n",
    "Add some random noise to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3344d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard deviation for noise to be added in the image\n",
    "sigma = 0.5\n",
    "\n",
    "# Add random noise to the image\n",
    "random_noise_image = random_noise(image, var=sigma ** 2)\n",
    "\n",
    "plt.imshow(random_noise_image)\n",
    "plt.title('Image with Random Noise')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0635b2ce",
   "metadata": {},
   "source": [
    "### Gaussian Blur\n",
    "\n",
    "Perform gaussian blur on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a824ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "blurred_image = gaussian(image, sigma=5, multichannel=True)\n",
    "\n",
    "plt.imshow(blurred_image)\n",
    "plt.title('Blurred Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86010edc",
   "metadata": {},
   "source": [
    "In the lecture, we explored more types of augmentations. Those augmentations came from the `imgaug` library which we will download by typing the command below in your command prompt/terminal:\n",
    "\n",
    "`conda install imgaug`\n",
    "\n",
    "See the documentation [here](https://github.com/aleju/imgaug)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2ebb98",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d743a3",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We will use the Census Income dataset as our dataset. Let's load it in a `DataFrame`.\n",
    "\n",
    "### Pipeline\n",
    "\n",
    "We will perform the following steps to our data.\n",
    "\n",
    "- Pre-processing\n",
    "    - Data Loading\n",
    "    - Feature Encoding\n",
    "    - Data Scaling\n",
    "- Training a linear regression model\n",
    "- Hyperparameter Tuning: Learning Rate and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4302342c",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "Let's load `census_income.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52849263",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('census_income.csv')\n",
    "df.columns = ['age', 'workclass', 'fnlwgt', 'education', 'educationnum', \n",
    "              'maritalstatus', 'occupation', 'relationship', 'race', 'sex',\n",
    "              'capitalgain','capitalloss', 'hoursperweek', 'nativecountry', \n",
    "              'label']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eae7c01",
   "metadata": {},
   "source": [
    "You will normally use EDA and feature selection to select the features for your project. But for the purpose of this notebook, let's just select few feature to apply feature encoding and scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26991866",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['age', 'fnlwgt', 'race', 'sex', 'educationnum', \n",
    "                   'capitalgain', 'capitalloss', 'hoursperweek', 'label']\n",
    "df = df[feature_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c33faf4",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "We will apply one hot encoding to `race`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a485438",
   "metadata": {},
   "source": [
    "Instantiate a `OneHotEncoder` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12a2101",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder_race = sklearn.preprocessing.OneHotEncoder(dtype='int8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf710d5",
   "metadata": {},
   "source": [
    "Fit the `race` feature by calling the `fit()` function of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8cdbb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "one_hot_encoder_race.fit(df['race'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c97154",
   "metadata": {},
   "source": [
    "Display the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1f5047",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder_race.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff360c9f",
   "metadata": {},
   "source": [
    "The encoding will then be an array with 5 columns, where the columns represents:\n",
    "- column 1 - ` Amer-Indian-Eskimo`\n",
    "- column 2 - ` Asian-Pac-Islander`\n",
    "- column 3 - ` Black`\n",
    "- column 4 - ` Other`\n",
    "- column 5 - ` White`\n",
    "\n",
    "If the instance has a value ` Asian-Pac-Islander` for the `race` feature, then the one-hot encoding for this instance is `[0, 1, 0, 0, 0]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabd0d27",
   "metadata": {},
   "source": [
    "Transform the `race` feature by calling the `transform()` function of the object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4b3d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = one_hot_encoder_race.transform(df['race'].values.reshape(-1, 1)).toarray()\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba1e5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_df = pd.DataFrame(encoding, columns=['race_' + x for x in one_hot_encoder_race.categories_])\n",
    "race_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bf0f8b",
   "metadata": {},
   "source": [
    "Concatenate the encoding to the original `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb84a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df, race_df], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c80827",
   "metadata": {},
   "source": [
    "### Ordinal Encoding\n",
    "Let's apply ordinal encoding for the `sex` feature.\n",
    "\n",
    "Technically, we should apply use one-hot encoding for this, but one-hot encoding is not needed for binary features (i.e., features with only two possible values).\n",
    "\n",
    "Let's create a dictionary that will map string values in the `sex` feature to its corresponding integer value. See list below:\n",
    "- ` Male` - `0`\n",
    "- ` Female` - `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac3ae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_mapping_dict = {\n",
    "    ' Male' : 0,\n",
    "    ' Female': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860f59bf",
   "metadata": {},
   "source": [
    "Use the `map()` function to transform the `sex` feature to its corresponding integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30c208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sex'] = df['sex'].map(sex_mapping_dict)\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5b3ae4",
   "metadata": {},
   "source": [
    "Rename columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fc4813",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['age', 'fnlwgt', 'race', 'sex', 'educationnum', \n",
    "            'capitalgain', 'capitalloss', 'hoursperweek', 'label', \n",
    "            'race_american_indian_eskimo', 'race_api', 'race_black',\n",
    "            'race_other', 'race_white']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aba4fb6",
   "metadata": {},
   "source": [
    "Drop the `race` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aaf05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['race'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899d1934",
   "metadata": {},
   "source": [
    "### Train/test split\n",
    "\n",
    "Let's split the dataset into train, validation, and test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba6113c",
   "metadata": {},
   "source": [
    "First, remove the `label` feature from `X` since this is our target feature. We will instead store it in `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bd1f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['label']).values\n",
    "y = df['label'].values\n",
    "\n",
    "print('X ', X.shape)\n",
    "print('y ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e21adf",
   "metadata": {},
   "source": [
    "Import `train_test_split()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d9e3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f4dd4f",
   "metadata": {},
   "source": [
    "Divide the dataset into train and test sets, where 20% of the data will be placed in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0d1ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1914cb5b",
   "metadata": {},
   "source": [
    "We will get 10% from the train set to produce a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34529ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8132e4b3",
   "metadata": {},
   "source": [
    "Let's display the shape of the train, validation, and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fae6dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train', X_train.shape)\n",
    "print('y_train', y_train.shape)\n",
    "\n",
    "print('X_val', X_val.shape)\n",
    "print('y_val', y_val.shape)\n",
    "\n",
    "print('X_test', X_test.shape)\n",
    "print('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c989d82",
   "metadata": {},
   "source": [
    "### Simple pipeline\n",
    "\n",
    "In this section, we will create a simple training pipeline using `sklearn.pipeline.Pipeline` and a classifier `sklearn.tree.DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46767157",
   "metadata": {},
   "source": [
    "Import `sklearn.pipeline.Pipeline` and `sklearn.tree.DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322cecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050ea996",
   "metadata": {},
   "source": [
    "Instantiate a `Pipeline` object. We will need to pass a list of transforms and a final estimator. Each element in the list is a tuple `(name, transform)`. In the example below, we have 2 elements in the list, where the `name`s of the elements are `scaler` and `classifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb4c244",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('scaler', sklearn.preprocessing.StandardScaler()),\n",
    "    ('classifier', DecisionTreeClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35bb815",
   "metadata": {},
   "source": [
    "Execute the pipeline by calling the `fit()` function of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc708ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd5e11a",
   "metadata": {},
   "source": [
    "### Predict on train, validation, and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8bbaa1",
   "metadata": {},
   "source": [
    "Evaluate the model on the train set by calling the `score()` function to get the train accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ed4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5632b4f",
   "metadata": {},
   "source": [
    "Evaluate the model on the validation set by calling the `score()` function to get the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d93590",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d0c582",
   "metadata": {},
   "source": [
    "Evaluate the model on the test set by calling the `score()` function to get the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23312ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da0b42c",
   "metadata": {},
   "source": [
    "To know the model hyperparameters used in the pipeline you can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edcc3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac80e67",
   "metadata": {},
   "source": [
    "## Simple pipeline with random search of hyperparameters\n",
    "\n",
    "In this section, we will integrate cross-validation in our pipeline to search the best set of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8df0872",
   "metadata": {},
   "source": [
    "Import `sklearn.model_selection.RandomizedSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353954ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7bd1e2",
   "metadata": {},
   "source": [
    "Instantiate a `Pipeline` object with an `sklearn.preprocessing.StandardScaler` and a `DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874652a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('scaler', sklearn.preprocessing.StandardScaler()),\n",
    "    ('classifier', DecisionTreeClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0f83f8",
   "metadata": {},
   "source": [
    "Create a list of dictionaries of the different hyperparameters that we want to try. In the example below, we indicated different values for the `DecisionTreeClassifier` `criterion`, `min_impurity_split`, and `max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6410bfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    {\n",
    "        'classifier__criterion': ['gini', 'entropy'],\n",
    "        'classifier__min_impurity_decrease': [0.001, 0.01, 0.05, 0.1, 0.3, 0.5],\n",
    "        'classifier__max_depth': [5, 10, 20, 30]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fb56c8",
   "metadata": {},
   "source": [
    "Instantiate a `RandomizedSearchCV` object. Pass the `Pipeline` object and the varible `parameters`. This performs randomized search on the different values of the hyperparameters that we listed in the variable `parameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b6faf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rscv = RandomizedSearchCV(pipe, parameters, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2821807d",
   "metadata": {},
   "source": [
    "Execute the randomized search by calling the `fit()` function of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdeb229",
   "metadata": {},
   "outputs": [],
   "source": [
    "rscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9bd0fe",
   "metadata": {},
   "source": [
    "By default, RandomizedSearchCV will act as the model with the best found parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219972e4",
   "metadata": {},
   "source": [
    "Get the prediction of the model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3db9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rscv.predict(X_train)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d68b1ec",
   "metadata": {},
   "source": [
    "Compute for the accuracy of the model on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e926fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(predictions == y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02870071",
   "metadata": {},
   "source": [
    "Get the prediction of the model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f58b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rscv.predict(X_val)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc98cb97",
   "metadata": {},
   "source": [
    "Compute for the accuracy of the model on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bb4957",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(predictions == y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450f3eac",
   "metadata": {},
   "source": [
    "Get the prediction of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d75e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rscv.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777bf31b",
   "metadata": {},
   "source": [
    "Compute for the accuracy of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aa171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(predictions == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43bda10",
   "metadata": {},
   "source": [
    "Here are the best parameters found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd742679",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rscv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e68bdd",
   "metadata": {},
   "source": [
    "## Pipeline with different classifiers + random search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85692dc1",
   "metadata": {},
   "source": [
    "`sklearn.pipeline.Pipeline` cannot handle multiple claassifiers by default, but we can make a class that switches the classifier for us.\n",
    "\n",
    "source: https://stackoverflow.com/questions/48507651/multiple-classification-models-in-a-scikit-pipeline-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574315c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class ClassifierSwitcher(BaseEstimator):\n",
    "\n",
    "    def __init__(self, estimator=RandomForestClassifier()):\n",
    "        '''\n",
    "        A Custom BaseEstimator that can switch between classifiers.\n",
    "        :param estimator: sklearn object - The classifier\n",
    "        '''\n",
    "\n",
    "        self.estimator = estimator\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        self.estimator.fit(X, y)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        return self.estimator.predict(X)\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.estimator.predict_proba(X)\n",
    "\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.estimator.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e79ab82",
   "metadata": {},
   "source": [
    "Import `sklearn.ensemble.AdaBoostClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e7dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec1f460",
   "metadata": {},
   "source": [
    "Here, we have a pipeline that experiments between a random forest classifier and an adaboost classifier, and we also list the hyperparameters we want it to tweak in random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa781de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('classifier', ClassifierSwitcher()),\n",
    "])\n",
    "\n",
    "parameters = [\n",
    "    {\n",
    "        'classifier__estimator': [RandomForestClassifier()], # SVM if hinge loss / logreg if log loss\n",
    "        'classifier__estimator__criterion': ['gini', 'entropy'],\n",
    "        'classifier__estimator__min_impurity_decrease': [0.001, 0.01, 0.05, 0.1, 0.3, 0.5],\n",
    "        'classifier__estimator__max_depth': [5, 10, 20, 30]\n",
    "    },\n",
    "    {\n",
    "        'classifier__estimator': [AdaBoostClassifier()],\n",
    "        'classifier__estimator__n_estimators': [100, 150, 200, 250, 300],\n",
    "        'classifier__estimator__learning_rate': [0.001, 0.01, 0.1, 1]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c364be12",
   "metadata": {},
   "source": [
    "Instantiate a `RandomizedSearchCV` object. Pass the `Pipeline` object and the varible `parameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977bd7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rscv = RandomizedSearchCV(pipeline, parameters, cv=5, n_jobs=12, verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9611a170",
   "metadata": {},
   "source": [
    "Execute the randomized search by calling the `fit()` function of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe3acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6876c342",
   "metadata": {},
   "source": [
    "Compute for the accuracy of the model on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac1d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rscv.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b66903",
   "metadata": {},
   "source": [
    "Compute for the accuracy of the model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d342fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "rscv.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981eaf34",
   "metadata": {},
   "source": [
    "Compute for the accuracy of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89114b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rscv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26149cd1",
   "metadata": {},
   "source": [
    "Here are the best parameters found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0ba772",
   "metadata": {},
   "outputs": [],
   "source": [
    "rscv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da614167",
   "metadata": {},
   "source": [
    "Hope this will help you with your project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34b511e",
   "metadata": {},
   "source": [
    "## <center>fin</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
